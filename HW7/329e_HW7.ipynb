{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "\n",
    "- Tori Garfield teg755\n",
    "- Veronica Alejandro vaa678\n",
    "\n",
    "## Supprt Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers here if you wish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Enable inline mode for matplotlib so that Jupyter displays graphs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your allowed to use only the above libraries that are imported. No other libs should be used in this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Dataset \n",
    "\n",
    "In this Assignment we will work with some patients dataset. \n",
    "\n",
    "We have access to 303 patients data. The features are listed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPain</th>\n",
       "      <th>RestBP</th>\n",
       "      <th>Chol</th>\n",
       "      <th>Fbs</th>\n",
       "      <th>RestECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExAng</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Thal</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>138</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Sex     ChestPain  RestBP  Chol  Fbs  RestECG  MaxHR  ExAng  \\\n",
       "0     63    1       typical     145   233    1        2    150      0   \n",
       "1     67    1  asymptomatic     160   286    0        2    108      1   \n",
       "2     67    1  asymptomatic     120   229    0        2    129      1   \n",
       "3     37    1    nonanginal     130   250    0        0    187      0   \n",
       "4     41    0    nontypical     130   204    0        2    172      0   \n",
       "..   ...  ...           ...     ...   ...  ...      ...    ...    ...   \n",
       "298   45    1       typical     110   264    0        0    132      0   \n",
       "299   68    1  asymptomatic     144   193    1        0    141      0   \n",
       "300   57    1  asymptomatic     130   131    0        0    115      1   \n",
       "301   57    0    nontypical     130   236    0        2    174      0   \n",
       "302   38    1    nonanginal     138   175    0        0    173      0   \n",
       "\n",
       "     Oldpeak  Slope   Ca        Thal Target  \n",
       "0        2.3      3  0.0       fixed     No  \n",
       "1        1.5      2  3.0      normal    Yes  \n",
       "2        2.6      2  2.0  reversable    Yes  \n",
       "3        3.5      3  0.0      normal     No  \n",
       "4        1.4      1  0.0      normal     No  \n",
       "..       ...    ...  ...         ...    ...  \n",
       "298      1.2      2  0.0  reversable    Yes  \n",
       "299      3.4      2  2.0  reversable    Yes  \n",
       "300      1.2      2  1.0  reversable    Yes  \n",
       "301      0.0      2  1.0      normal    Yes  \n",
       "302      0.0      1  NaN      normal     No  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "heart_df = pd.read_csv(\"Heart.csv\")\n",
    "heart_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age:** The person’s age in years\n",
    "\n",
    "**Sex:** The person’s sex (1 = male, 0 = female)\n",
    "\n",
    "**ChestPain:** chest pain type\n",
    "\n",
    "* Value 0: asymptomatic\n",
    "* Value 1: atypical angina\n",
    "* Value 2: non-anginal pain\n",
    "* Value 3: typical angina\n",
    "\n",
    "**RestBP:** The person’s resting blood pressure (mm Hg on admission to the hospital)\n",
    "\n",
    "**Chol:** The person’s cholesterol measurement in mg/dl\n",
    "\n",
    "**Fbs:** The person’s fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n",
    "restecg: resting electrocardiographic results\n",
    "\n",
    "* Value 0: showing probable or definite left ventricular hypertrophy by Estes’ criteria\n",
    "* Value 1: normal\n",
    "* Value 2: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "\n",
    "**RestECG:** The person’s maximum heart rate achieved\n",
    "\n",
    "**MaxHR:** Exercise induced angina (1 = yes; 0 = no)\n",
    "\n",
    "**Oldpeak:** ST depression induced by exercise relative to rest (‘ST’ relates to positions on the ECG plot. See more here)\n",
    "\n",
    "**Slope:** the slope of the peak exercise ST segment — 0: downsloping; 1: flat; 2: upsloping\n",
    "\n",
    "* 0: downsloping; \n",
    "* 1: flat; \n",
    "* 2: upsloping\n",
    "\n",
    "**Ca:** The number of major vessels (0–3)\n",
    "\n",
    "**Thal:** A blood disorder called thalassemia Value 0: NULL (dropped from the dataset previously\n",
    "\n",
    "* Value 1: fixed defect (no blood flow in some part of the heart)\n",
    "* Value 2: normal blood flow\n",
    "* Value 3: reversible defect (a blood flow is observed but it is not normal)\n",
    "\n",
    "**Target:** Heart disease (1 = no, 0= yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - 1 Implement SVM using libraries (4 points)\n",
    "We want to use **Suppert Vector Machine** to perdict if the patients will have heart problems or not. The column \"Target\" in our datasets includes data about heart diseases. If the patient had heart disease we have a 1 and if not a zero. \n",
    "\n",
    "Prepare your data set for predicting heart disease (\"Target\" column) out of 3 features:\n",
    "\n",
    "* Age of the patient (Column **\"Age\"**)\n",
    "* Gender of the patient (male or female - Column **\"Sex\"**)\n",
    "* Cholestrol level of the patient (Column **\"Chol\"**) \n",
    "\n",
    "\n",
    "Split your data into 80% traning data and 20% test data, and implement Support Vector Machine using Scikit-Learn. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-20280e6c9588>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Target'] = df['Target'].apply(lambda x : 1 if x == 'Yes' else 0)\n"
     ]
    }
   ],
   "source": [
    "# Add your code Here! \n",
    "df = heart_df[['Age', 'Sex', 'Chol', 'Target']]\n",
    "df['Target'] = df['Target'].apply(lambda x : 1 if x == 'Yes' else 0)\n",
    "myX = df[['Age', 'Sex', 'Chol']]\n",
    "target = df['Target']\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(myX, target, test_size = 0.2, stratify = target)\n",
    "\n",
    "model = svm.SVC()\n",
    "model.fit(xTrain, yTrain)\n",
    "\n",
    "yPred = model.predict(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - (4 points)\n",
    "\n",
    "Cacluate the accuracy, Precision, Recall and F1 score of your **SVM** implementaion from Task 1. \n",
    "Print the results. \n",
    "\n",
    "You may use library methods for this task if you choose to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5737704918032787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.91      0.70        33\n",
      "           1       0.62      0.18      0.28        28\n",
      "\n",
      "    accuracy                           0.57        61\n",
      "   macro avg       0.60      0.54      0.49        61\n",
      "weighted avg       0.59      0.57      0.50        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add your code Here! \n",
    "#calculate accuracy\n",
    "print(\"Accuracy Score:\", accuracy_score(yTest, yPred))\n",
    "\n",
    "#calculate precision, recall, and F1 score\n",
    "print(classification_report(yTest,yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Implement SVM without using libraries  - (4 points)\n",
    "\n",
    "Implement SVM from scratch using Hinge Loss function and Gradient Descent. \n",
    "Try to produce the same result as you get from the libraries. \n",
    "\n",
    "\n",
    "* Do as many iterations as needed \n",
    "* Do maximum **100 iterations**\n",
    "* Use a very small learning rate for checking your GD implementation. \n",
    "* Your are allowed to use your choice of learning rate, like using 0.0001, 0.001 or 0.01 or 0.1 or higher. \n",
    "* Visualize your costs. \n",
    "* No need to add an y-intercept in this task. \n",
    "* You can use libraries to report accuracy, Precision, Recall and F1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = yTrain.to_numpy() #Do not run again -- yTrain is now an array\n",
    "xTrain = xTrain.to_numpy() #Do not run again -- xTrain is now an array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Cost is: 0.01 weights [0. 0. 0.]\n",
      "Epoch 2 Cost is: 0.0086199515503398 weights [2.61487603e-04 3.59504132e-06 1.14557851e-03]\n",
      "Epoch 3 Cost is: 0.007242661817530469 weights [5.22713719e-04 7.18648760e-06 2.29001145e-03]\n",
      "Epoch 4 Cost is: 0.006011972203193894 weights [7.83678609e-04 1.07743424e-05 3.43329995e-03]\n",
      "Epoch 5 Cost is: 0.005605645032778375 weights [9.74175922e-04 1.35734855e-05 4.18850301e-03]\n",
      "Epoch 6 Cost is: 0.005535487233712337 weights [1.05382158e-03 1.48409037e-05 4.46712443e-03]\n",
      "Epoch 7 Cost is: 0.005505040298648476 weights [1.10780908e-03 1.56938314e-05 4.64666557e-03]\n",
      "Epoch 8 Cost is: 0.005490140675300381 weights [1.14513102e-03 1.62979723e-05 4.76672964e-03]\n",
      "Epoch 9 Cost is: 0.005479657457142713 weights [1.17774622e-03 1.68601867e-05 4.87026870e-03]\n",
      "Epoch 10 Cost is: 0.005471381076347602 weights [1.20570071e-03 1.73391943e-05 4.95758851e-03]\n",
      "Epoch 11 Cost is: 0.005465280626388834 weights [1.23102393e-03 1.77764005e-05 5.03709373e-03]\n",
      "Epoch 12 Cost is: 0.005461118371296194 weights [1.25206564e-03 1.81305250e-05 5.10098226e-03]\n",
      "Epoch 13 Cost is: 0.005457609253626518 weights [1.27077225e-03 1.84429730e-05 5.15720359e-03]\n",
      "Epoch 14 Cost is: 0.005454107150683064 weights [1.28946015e-03 1.87551085e-05 5.21336870e-03]\n",
      "Epoch 15 Cost is: 0.005451562471720434 weights [1.30812937e-03 1.90669319e-05 5.26947765e-03]\n",
      "Epoch 16 Cost is: 0.005450269823695485 weights [1.31901133e-03 1.92544765e-05 5.30375362e-03]\n",
      "Epoch 17 Cost is: 0.0054490636256895545 weights [1.32988240e-03 1.94418336e-05 5.33799533e-03]\n",
      "Epoch 18 Cost is: 0.0054482582447492565 weights [1.33896574e-03 1.95876810e-05 5.36488874e-03]\n",
      "Epoch 19 Cost is: 0.00544745447376546 weights [1.34803999e-03 1.97333826e-05 5.39175525e-03]\n",
      "Epoch 20 Cost is: 0.005446652309519858 weights [1.35710518e-03 1.98789385e-05 5.41859490e-03]\n",
      "Epoch 21 Cost is: 0.005445851748800583 weights [1.36616130e-03 2.00243488e-05 5.44540771e-03]\n",
      "Epoch 22 Cost is: 0.005445052788402187 weights [1.37520836e-03 2.01696137e-05 5.47219371e-03]\n",
      "Epoch 23 Cost is: 0.005444255425125628 weights [1.38424637e-03 2.03147334e-05 5.49895292e-03]\n",
      "Epoch 24 Cost is: 0.005443459655778256 weights [1.39327535e-03 2.04597079e-05 5.52568537e-03]\n",
      "Epoch 25 Cost is: 0.005442668604946594 weights [1.40229530e-03 2.06045374e-05 5.55239109e-03]\n",
      "Epoch 26 Cost is: 0.005442279183378158 weights [1.40948804e-03 2.07078998e-05 5.57208663e-03]\n",
      "Epoch 27 Cost is: 0.005442091505319066 weights [1.41411161e-03 2.08111589e-05 5.58498562e-03]\n",
      "Epoch 28 Cost is: 0.005441904202428414 weights [1.41873056e-03 2.09143147e-05 5.59787171e-03]\n",
      "Epoch 29 Cost is: 0.005441717273956242 weights [1.42334489e-03 2.10173673e-05 5.61074491e-03]\n",
      "Epoch 30 Cost is: 0.005441530719154084 weights [1.42795460e-03 2.11203169e-05 5.62360524e-03]\n",
      "Epoch 31 Cost is: 0.005441344537274978 weights [1.43255970e-03 2.12231635e-05 5.63645271e-03]\n",
      "Epoch 32 Cost is: 0.005441197517602441 weights [1.43716020e-03 2.13259073e-05 5.64928733e-03]\n",
      "Epoch 33 Cost is: 0.005441153816650485 weights [1.44010321e-03 2.13872260e-05 5.65520829e-03]\n",
      "Epoch 34 Cost is: 0.005441110203056733 weights [1.44304327e-03 2.14484834e-05 5.66112333e-03]\n",
      "Epoch 35 Cost is: 0.005441066676646554 weights [1.44598039e-03 2.15096795e-05 5.66703246e-03]\n",
      "Epoch 36 Cost is: 0.00544102323724567 weights [1.44891458e-03 2.15708145e-05 5.67293567e-03]\n",
      "Epoch 37 Cost is: 0.005440979884680146 weights [1.45184583e-03 2.16318883e-05 5.67883299e-03]\n",
      "Epoch 38 Cost is: 0.005440936618776404 weights [1.45477415e-03 2.16929010e-05 5.68472440e-03]\n",
      "Epoch 39 Cost is: 0.0054408934393612015 weights [1.45769954e-03 2.17538528e-05 5.69060992e-03]\n",
      "Epoch 40 Cost is: 0.00544085034626165 weights [1.46062200e-03 2.18147435e-05 5.69648956e-03]\n",
      "Epoch 41 Cost is: 0.005440807339305205 weights [1.46354155e-03 2.18755734e-05 5.70236332e-03]\n",
      "Epoch 42 Cost is: 0.005440764418319666 weights [1.46645817e-03 2.19363425e-05 5.70823121e-03]\n",
      "Epoch 43 Cost is: 0.005440721583133178 weights [1.46937188e-03 2.19970508e-05 5.71409322e-03]\n",
      "Epoch 44 Cost is: 0.005440678833574224 weights [1.47228267e-03 2.20576983e-05 5.71994938e-03]\n",
      "Epoch 45 Cost is: 0.005440636169471642 weights [1.47519055e-03 2.21182853e-05 5.72579968e-03]\n",
      "Epoch 46 Cost is: 0.0054405935906546 weights [1.47809553e-03 2.21788116e-05 5.73164412e-03]\n",
      "Epoch 47 Cost is: 0.0054405510969526135 weights [1.48099760e-03 2.22392774e-05 5.73748273e-03]\n",
      "Epoch 48 Cost is: 0.005440508688195536 weights [1.48389676e-03 2.22996828e-05 5.74331549e-03]\n",
      "Epoch 49 Cost is: 0.005440466364213566 weights [1.48679303e-03 2.23600277e-05 5.74914243e-03]\n",
      "Epoch 50 Cost is: 0.005440424124837234 weights [1.48968641e-03 2.24203123e-05 5.75496353e-03]\n",
      "Epoch 51 Cost is: 0.005440381969897417 weights [1.49257688e-03 2.24805366e-05 5.76077882e-03]\n",
      "Epoch 52 Cost is: 0.005440339899225325 weights [1.49546447e-03 2.25407007e-05 5.76658828e-03]\n",
      "Epoch 53 Cost is: 0.005440297912652504 weights [1.49834917e-03 2.26008047e-05 5.77239194e-03]\n",
      "Epoch 54 Cost is: 0.005440256010010844 weights [1.50123099e-03 2.26608485e-05 5.77818980e-03]\n",
      "Epoch 55 Cost is: 0.005440214191132565 weights [1.50410992e-03 2.27208323e-05 5.78398186e-03]\n",
      "Epoch 56 Cost is: 0.005440172455850222 weights [1.50698598e-03 2.27807561e-05 5.78976812e-03]\n",
      "Epoch 57 Cost is: 0.005440130803996709 weights [1.50985916e-03 2.28406199e-05 5.79554860e-03]\n",
      "Epoch 58 Cost is: 0.005440089235405252 weights [1.51272946e-03 2.29004239e-05 5.80132330e-03]\n",
      "Epoch 59 Cost is: 0.00544004774990941 weights [1.51559690e-03 2.29601682e-05 5.80709223e-03]\n",
      "Epoch 60 Cost is: 0.005440006347343071 weights [1.51846147e-03 2.30198526e-05 5.81285538e-03]\n",
      "Epoch 61 Cost is: 0.005439965027540464 weights [1.52132317e-03 2.30794774e-05 5.81861278e-03]\n",
      "Epoch 62 Cost is: 0.005439923790336144 weights [1.52418201e-03 2.31390425e-05 5.82436441e-03]\n",
      "Epoch 63 Cost is: 0.005439882635564993 weights [1.52703800e-03 2.31985481e-05 5.83011030e-03]\n",
      "Epoch 64 Cost is: 0.00543984156306223 weights [1.52989113e-03 2.32579942e-05 5.83585043e-03]\n",
      "Epoch 65 Cost is: 0.0054398005726634 weights [1.53274140e-03 2.33173808e-05 5.84158483e-03]\n",
      "Epoch 66 Cost is: 0.005439759664204378 weights [1.53558882e-03 2.33767081e-05 5.84731349e-03]\n",
      "Epoch 67 Cost is: 0.005439718837521365 weights [1.53843340e-03 2.34359760e-05 5.85303643e-03]\n",
      "Epoch 68 Cost is: 0.005439678092450893 weights [1.54127513e-03 2.34951847e-05 5.85875364e-03]\n",
      "Epoch 69 Cost is: 0.005439637428829815 weights [1.54411402e-03 2.35543341e-05 5.86446513e-03]\n",
      "Epoch 70 Cost is: 0.005439596846495316 weights [1.54695007e-03 2.36134244e-05 5.87017092e-03]\n",
      "Epoch 71 Cost is: 0.005439556345284903 weights [1.54978329e-03 2.36724556e-05 5.87587099e-03]\n",
      "Epoch 72 Cost is: 0.005439515925036411 weights [1.55261367e-03 2.37314278e-05 5.88156537e-03]\n",
      "Epoch 73 Cost is: 0.005439475585587995 weights [1.55544122e-03 2.37903410e-05 5.88725405e-03]\n",
      "Epoch 74 Cost is: 0.005439435326778137 weights [1.55826595e-03 2.38491953e-05 5.89293705e-03]\n",
      "Epoch 75 Cost is: 0.005439395148445638 weights [1.56108785e-03 2.39079907e-05 5.89861436e-03]\n",
      "Epoch 76 Cost is: 0.005439355050429627 weights [1.56390692e-03 2.39667273e-05 5.90428599e-03]\n",
      "Epoch 77 Cost is: 0.0054393150325695505 weights [1.56672318e-03 2.40254052e-05 5.90995195e-03]\n",
      "Epoch 78 Cost is: 0.0054392750947051755 weights [1.56953662e-03 2.40840245e-05 5.91561225e-03]\n",
      "Epoch 79 Cost is: 0.005439235236676592 weights [1.57234725e-03 2.41425851e-05 5.92126688e-03]\n",
      "Epoch 80 Cost is: 0.005439195458324208 weights [1.57515507e-03 2.42010871e-05 5.92691587e-03]\n",
      "Epoch 81 Cost is: 0.00543915575948875 weights [1.57796008e-03 2.42595306e-05 5.93255920e-03]\n",
      "Epoch 82 Cost is: 0.005439116140011263 weights [1.58076229e-03 2.43179157e-05 5.93819689e-03]\n",
      "Epoch 83 Cost is: 0.005439076599733113 weights [1.58356169e-03 2.43762424e-05 5.94382894e-03]\n",
      "Epoch 84 Cost is: 0.005439037138495978 weights [1.58635829e-03 2.44345108e-05 5.94945536e-03]\n",
      "Epoch 85 Cost is: 0.005438997756141857 weights [1.58915210e-03 2.44927210e-05 5.95507615e-03]\n",
      "Epoch 86 Cost is: 0.005438958452513061 weights [1.59194311e-03 2.45508729e-05 5.96069132e-03]\n",
      "Epoch 87 Cost is: 0.005438919227452219 weights [1.59473133e-03 2.46089666e-05 5.96630088e-03]\n",
      "Epoch 88 Cost is: 0.005438880080802275 weights [1.59751677e-03 2.46670023e-05 5.97190482e-03]\n",
      "Epoch 89 Cost is: 0.005438841012406483 weights [1.60029942e-03 2.47249799e-05 5.97750317e-03]\n",
      "Epoch 90 Cost is: 0.005438802022108413 weights [1.60307928e-03 2.47828995e-05 5.98309591e-03]\n",
      "Epoch 91 Cost is: 0.00543876310975195 weights [1.60585637e-03 2.48407613e-05 5.98868306e-03]\n",
      "Epoch 92 Cost is: 0.005438724275181289 weights [1.60863068e-03 2.48985651e-05 5.99426463e-03]\n",
      "Epoch 93 Cost is: 0.005438685518240933 weights [1.61140221e-03 2.49563112e-05 5.99984061e-03]\n",
      "Epoch 94 Cost is: 0.0054386468387757 weights [1.61417098e-03 2.50139995e-05 6.00541102e-03]\n",
      "Epoch 95 Cost is: 0.0054386082366307195 weights [1.61693697e-03 2.50716302e-05 6.01097586e-03]\n",
      "Epoch 96 Cost is: 0.005438569711651428 weights [1.61970020e-03 2.51292032e-05 6.01653513e-03]\n",
      "Epoch 97 Cost is: 0.005438531263683568 weights [1.62246066e-03 2.51867186e-05 6.02208884e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 Cost is: 0.0054384928925731955 weights [1.62521837e-03 2.52441765e-05 6.02763700e-03]\n",
      "Epoch 99 Cost is: 0.005438454598166675 weights [1.62797331e-03 2.53015769e-05 6.03317961e-03]\n",
      "Epoch 100 Cost is: 0.005438416380310671 weights [1.63072551e-03 2.53589200e-05 6.03871668e-03]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_cost(X, y, W, regularization_factor):\n",
    "    #this function calculates the hinge loss.\n",
    "    #primal problem in SVM\n",
    "    n = X.shape[0]\n",
    "    distances = 1 - y * (np.dot(X, W))\n",
    "    \n",
    "    # This is our max(0, distance). \n",
    "    distances[distances < 0] = 0\n",
    "    \n",
    "    hinge_loss = regularization_factor * (np.sum(distances) / n)\n",
    "    return (1 / 2 * np.dot(W, W) + hinge_loss)\n",
    "\n",
    "def calculate_gradient(X, y, W, regularization_factor):\n",
    "        \n",
    "    distance = 1 - (y * np.dot(X, W))\n",
    "    dw = np.zeros(len(W))\n",
    "    \n",
    "    for ind, d in enumerate(distance):\n",
    "\n",
    "        if (d < 0):\n",
    "            di = W\n",
    "        else:\n",
    "            di = W - (regularization_factor * y[ind] * X[ind])\n",
    "        \n",
    "        dw += di\n",
    "    \n",
    "    dw = dw/len(y) \n",
    "    \n",
    "    return dw\n",
    "\n",
    "weights = np.zeros(3)\n",
    "num_iterations = 100\n",
    "learning_rate = .001\n",
    "regularization = 0.01\n",
    "\n",
    "cost_list = []\n",
    "\n",
    "for i in range(0, num_iterations):\n",
    "    cost = compute_cost(xTrain, yTrain, weights, regularization)\n",
    "    print(\"Epoch\", i + 1, \"Cost is:\", cost, \"weights\", weights)\n",
    "    cost_list.append(cost)\n",
    "    grad = calculate_gradient(xTrain, yTrain, weights, regularization)\n",
    "    weights = weights - learning_rate * grad\n",
    "\n",
    "plt.plot(np.arange(num_iterations), cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.45901639344262296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        33\n",
      "           1       0.46      1.00      0.63        28\n",
      "\n",
      "    accuracy                           0.46        61\n",
      "   macro avg       0.23      0.50      0.31        61\n",
      "weighted avg       0.21      0.46      0.29        61\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veron\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def predict_svm(weights, X):\n",
    "    \"\"\"\n",
    "    Predict the class between 0 and 1 using learned SVM parameters weights.\n",
    "    \"\"\"    \n",
    "    return np.where(np.dot(X, weights) >= 0.5, 1, 0)\n",
    "\n",
    "# calculate accuracy\n",
    "print(\"Accuracy Score:\", accuracy_score(yTest, predict_svm(weights, xTest)))\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "\n",
    "print(classification_report(yTest, predict_svm(weights, xTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Compare SVM results with Logistic Regression - (4 points)\n",
    "\n",
    "Which model performs better here? Compare your results wit the logistic regression. You can use libraries for this task, it is not necessary to implement logistic regression from sratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.6065573770491803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65        33\n",
      "           1       0.58      0.54      0.56        28\n",
      "\n",
      "    accuracy                           0.61        61\n",
      "   macro avg       0.60      0.60      0.60        61\n",
      "weighted avg       0.60      0.61      0.61        61\n",
      "\n",
      "The accuracy of SVM using libraries is 0.5573770491803278 where as the accuracy of the Logistic Regression model is 0.0.6065573770491803.\n",
      "This suggests that the Logistic Regression model performs better than the Support Vector Machine in terms of accuracy.\n",
      "The precision values are also higher in the Logisitic Regression model, where we get 63% and 58% for labels 0 and 1, respectively. These values are lower for SVM, which are 57% and 62% for labels 0 and 1, respectively\n",
      "However, we do get a higher recall value with the SVM for label 0, which is 91% but 67% in the logistic regression. For label 1, the logistic regression produces a higher recall value of 54% compared to 18%.\n",
      "Lastly, the F1-score is slightly higher in SVM regression for label 0, at 70% but at 65% for Logistic Regression. The F1-score for label 1 is higher in the logistic regression at 56% but at 28% for SVM.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model2 = LogisticRegression(penalty='none', fit_intercept=False)\n",
    "model2.fit(xTrain,yTrain)\n",
    "y_pred = model2.predict(xTest)\n",
    "\n",
    "print('Accuracy score: {}'.format(accuracy_score(yTest,y_pred)))\n",
    "\n",
    "print(classification_report(yTest,y_pred))\n",
    "print(\"The accuracy of SVM using libraries is 0.5573770491803278 where as the accuracy of the Logistic Regression model is 0.0.6065573770491803.\") \n",
    "print(\"This suggests that the Logistic Regression model performs better than the Support Vector Machine in terms of accuracy.\")\n",
    "print(\"The precision values are also higher in the Logisitic Regression model, where we get 63% and 58% for labels 0 and 1, respectively. These values are lower for SVM, which are 57% and 62% for labels 0 and 1, respectively\") \n",
    "print('However, we do get a higher recall value with the SVM for label 0, which is 91% but 67% in the logistic regression. For label 1, the logistic regression produces a higher recall value of 54% compared to 18%.')\n",
    "print('Lastly, the F1-score is slightly higher in SVM regression for label 0, at 70% but at 65% for Logistic Regression. The F1-score for label 1 is higher in the logistic regression at 56% but at 28% for SVM.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 5 - Apply a kernel function to improve SVM performance (4 points)\n",
    "\n",
    "Use the Scikit-learn librariy and apply a kernel function to improve the SVM performance. Check if this is possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for linear kernel: 0.6229508196721312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.55      0.61        33\n",
      "           1       0.57      0.71      0.63        28\n",
      "\n",
      "    accuracy                           0.62        61\n",
      "   macro avg       0.63      0.63      0.62        61\n",
      "weighted avg       0.64      0.62      0.62        61\n",
      "\n",
      "Accuracy score for poly kernel: 0.5409836065573771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.67      0.61        33\n",
      "           1       0.50      0.39      0.44        28\n",
      "\n",
      "    accuracy                           0.54        61\n",
      "   macro avg       0.53      0.53      0.53        61\n",
      "weighted avg       0.53      0.54      0.53        61\n",
      "\n",
      "Out of both accuracy scores between a linear kernel, RBF kernel (from task 1) and a poly kernel, we get a higher accuracy score using the linear kernel.\n"
     ]
    }
   ],
   "source": [
    "svclassifier = svm.SVC(kernel='linear')\n",
    "svclassifier.fit(xTrain, yTrain)\n",
    "y_pred2 = svclassifier.predict(xTest)\n",
    "\n",
    "print('Accuracy score for linear kernel: {}'.format(accuracy_score(yTest,y_pred2)))\n",
    "print(classification_report(yTest, y_pred2))\n",
    "\n",
    "svclassifier2 = svm.SVC(kernel='poly')\n",
    "svclassifier2.fit(xTrain, yTrain)\n",
    "y_pred3 = svclassifier2.predict(xTest)\n",
    "\n",
    "print('Accuracy score for poly kernel: {}'.format(accuracy_score(yTest,y_pred3)))\n",
    "print(classification_report(yTest, y_pred3))\n",
    "\n",
    "print('Out of both accuracy scores between a linear kernel, RBF kernel (from task 1) and a poly kernel, we get a higher accuracy score using the linear kernel.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
